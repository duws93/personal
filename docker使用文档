docker三个基本的概念
1：镜像(image)
2：容器(container)
3：仓库(repository)

docker -build  构建镜像
docker -ship   运输镜像，主要是在仓库之间进行
docker -run    运行镜像


1：namespace 命名空间
   是linux为我们提供的用于分离进程树，网络接口，挂载点以及进程间通信资源的方法
   docker通过命名空间完成对宿主机器进程和网络的隔离
   
2：网络
   docker启动的容器都具有单独的网络空间，默认的网络连接模式是网桥模式
   docker会为所有的启动的容器设置IP地址。默认docker服务器在宿主机器上启动创建
   新的虚拟网桥docker0,启动服务默认与其连接
   
   在默认情况下，每一个容器会创建两个一对虚拟网卡，两个虚拟网卡组成了数据的通道，
   其中一个会被放在创建的容器中，会加入到docker0网桥中。
   brctl show 获取当前网桥的接口
   
   docker0 会为每一个容器分配一个IP地址，并将docker0的IP地址设置为默认的网关，
   网桥docker0通过iptables中的配置与宿主机器上的网卡相连，所有符合条件的请求
   都会通过iptables转发到docker0并由网桥分发给对应的机器。
   iptables -t nat -L 获取docker链中新创建的规则。
   
   docker将容器的内部端口暴露给外部并对数据包进行转发，docker会分配一个IP地址。
   同时会向iptables中追加一条新的规则
   
   
3：libnetwork
   整个网络部分的功能都是通过docker拆分的libnetwork实现的，提供了实现连接不同容器。
   在网络容器模型中，每一个容器内部都包含一个sandbox，其中存储着当前的网络栈配置，
   每一个sandbox中都可能有一个或多个Endpoint，即虚拟网卡veth，sandbox通过Endpoint加入
   到不同的网络中，这里的网络可以是网桥或者VLAN.
   
   
4：挂载点
   限制docker启动的容器可以访问到宿主机器上的目录
   
   在新的进程中创建隔离的挂载点，子进程得到父进程挂载的拷贝。
   
   为了保证当前的容器进程无法访问宿主机器上其他目录，我们可以通过libcontainer提供的
   pivor_root或者chroot函数改变进程能够访问的文件目录的根节点。
   将需要的目录挂载到容器中，同时也禁止了当前容器去访问宿主机器的其他目录，保证了不同文件系统的隔离。
   
   
5：CGroups
   linux的命名空间解决了新创建的进程隔离文件系统，网络并于宿主机器之间的进程相互隔离。
   而对于物理资源上的隔离，比如CPU和内存。就通过CGroups来隔离宿主机器上的资源。
   
   CGroup是一组被相同的标准和参数限制的进程。不同的CGroup之间具有层级关系，
   可以从父类继承一些用于限制资源使用的标准和参数。
   
   在CGroup中，所有的任务就是一个系统的一个进程，而CGroup就是按照某种标准划分的进程。
   在CGroup机制中，所有的资源控制都是以CGroup为单位实现的，每一个进程都可以随时加入一个
   CGroup，也可以随时退出一个CGroup
   
   lssubsys -m 来获取当前CGroup中具有那些子系统。
   
6：UnionFS
   linux的命名空间解决进程，网络以及文件系统的隔离。
   CGroups解决了CPU,内存等物理资源的隔离。
   而UnionFS，设计用于把多个文件系统联合到同一个挂载点的文件系统服务。
   
   
7：存储驱动
   docker使用不同的存储驱动管理镜像内的文件系统并运行容器，这些存储驱动与
   docker volume存在区别，存储殷勤管理折能够在多个容器之间共享的内存。
   
8：docker构建并存储镜像
   docker中的每一个镜像都是由一系列的只读的层组成的。
   当容器被运行后，就会在镜像创建一个可读写的层，即容器层。所有对于容器的
   修改实际上是对这个容器读写层的修改。
   
   容器和镜像的区别在于，所有的镜像都是只读的，而每一个容器都是在镜像上加一个
   可读写的层，也就是同一个镜像可以对应多个容器。
   
   
9：容器，物理机，虚拟机的区别
   容器：容器虚拟化的是操作系统，共享同一套操作系统资源
   虚拟机：虚拟化出一套硬件后，在其上运行一套完整操作系统
   虚拟机的隔离程度高于容器。
   
   特性           容器                虚拟机
   启动           秒级                分钟
   硬盘使用       一般为MB            一般为GB
   性能           接近原生            弱于
   系统支持量     单机支持上前容器    一般几十个
   
   容器是一个应用层抽象，用于将代码和依赖资源打包在一起。 
       多个容器可以在同一台机器上运行，共享操作系统内核，
       但各自作为独立的进程在用户空间中运行。
       与虚拟机相比， 容器占用的空间较少（容器镜像大小通常只有几十兆），
       瞬间就能完成启动 。
    
   虚拟机（VM）是一个物理硬件层抽象，用于将一台服务器变成多台服务器。
       管理程序允许多个VM在一台机器上运行。
       每个VM都包含一整套操作系统、一个或多个应用、必要的二进制文件和库资源，
       因此占用大量空间。而且VM启动也十分缓慢 。



背景： node1不能访问外网， node2可以访问外网，node1通过node2的代理服务来访问外网。

1. node1不能访问外网
	1	vim /etc/resolv.conf
		注释掉DNS配置文件
	2. node2搭建代理服务器， 这里是在centos7.2上用Squid搭建HTTP代理服务器 [如果已经有其他代理服务器，可以忽略这步]

2.1 安装
	yum install  -y squid
	yum install  -y httpd-tools

2.2 生成密码文件
	mkdir /etc/squid/
	# pill 是用户名
	htpasswd -cd /etc/squid/passwords pill
	# 提示输入密码，在此pill设密码为 pill
	# 注意密码不要超过8位

2.3 测试密码文件
	
	/usr/lib64/squid/basic_ncsa_auth /etc/squid/passwords
	# 输入 用户名 密码
	pill pill
	# 提示OK说明成功，ERR是有问题，请检查一下之前步骤
	OK
	# 测试完成，crtl + c 打断

2.4 配置Squid

	vim /etc/squid/squid.conf 
	# 在最后添加
 
	auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/passwords
	auth_param basic realm proxy
	acl authenticated proxy_auth REQUIRED
	http_access allow authenticated
 
	# 这里是端口号，可以按需修改
	http_port 3128

2.5 启动Squid
	
	systemctl restart squid.service

 
3. 为docker设置代理

3.1 创建目录

	mkdir -p /etc/systemd/system/docker.service.d

3.2 创建文件/etc/systemd/system/docker.service.d/http-proxy.conf，内容如下：

	
	[Service]
	Environment="HTTP_PROXY=http://pill:pill@node2:3128/"

3.3 重启docker

	systemctl daemon-reload
	systemctl restart docker

3.4 验证docker代理是否设置成功

	systemctl show --property=Environment docker　　

显示如下结果说明设置成功
	Environment=GOTRACEBACK=crash HTTP_PROXY=http://pill:pill@node2:3128/
	
搜索指定版本	
yum list docker-ce.x86_64 --showduplicates | sort -r	
	
在仓库中搜索星数达到指定数目以上的镜像
docker search --filter=stars=10 mysql

不截断搜索镜像的描述
docker search --no-trunc=true mysql

显示那些定期自动重新构建的镜像
docker search --filter=is-automated=true mysql

运行镜像，并执行python的simpleHTTPServer模块
docker run -it -c 3 --cpuset-cpus="3" -p 80:80 
--name fedora_test --restart="on-failure:5" 
-w /var/www/html -v /var/www/html:/var/www/html 
fedora pyton -m SimpleHTTPServer 8080
 -c:指定获取CPU的优先级
 --cpuset-cpus:自定义在那个CPU上执行
 -w:指定工作目录
 -v:自定义将宿主机的目录挂载到容器的内部


docker 命令集

docker
	version 版本信息
	info    系统系统
	history  展示镜像创建历史
	inspect  查看镜像/容器的信息
	port     列出容器的端口映射
	ps       列出启动的容器
	attach   将另一个命令附加到正在运行的容器上
	exec     在正在运行的容器中运行命令
	cp       从容器拷贝文件到宿主机
					 docker cp containerid:path hostpath
					 docker cp hostpath containerid:path
					 容器无需启动，cp均可生效
	diff     检查容器启动后文件系统的变更信息
					 A -ADD, D -DELETE, C - CHANGE
	images   列出本地仓库镜像
	pull     拉取镜像
	push		 上传镜像
	save		 将镜像保存为tallbar
	load		 将tallbar文件加载为本地镜像
	export   从容器中将文件系统导出成为本地系统的tallbar文件
	search	 在仓库中获取镜像
	login		 登录仓库
	logout	 等出仓库
	tag			 给镜像添加别名
	rename	 修改镜像的别名
	stop		 停止容器
	pause		 暂停容器
	unpause  重启暂停容器
	kill   	 向容器发送kill 命令
	restart	 重启容器
	events	 查看docker服务器事件
	top			 获取容器内的进程
	logs		 查看由容器产生的日志
	stats		 查看容器CPU和内存使用
	wait		 查看容器直至它停止
	build		 构建镜像
	commit   从容器创建镜像
	create	 从镜像创建容器但不运行
	import	 将文件系统导入镜像
	
docker 运行时资源限制
Docker基于Linux内核提供的cgroups功能，
可以限制容器在运行时使用到的资源，比如内存、CPU、块I/O、网络等。
1：容器能够使用的内存和交换区大小
2：容器核心内存大小
3：容器虚拟内存的交换行为
4：容器内存的软性限制
5：是否杀死占用过多内存的容器
6：容器被杀死的优先级



查询全部的namespace
kubectl get ns

创建namespace
kubectl create namesapce dws

创建pod
kubectl apply -f xxx.yaml
kubectl create -f xxx.yaml

查看
kubectl get deployment -n dws

kubectl get pod -n dws

kubectl get rs -n dws

删除namespace
kubectl delete namespaces dws

删除deployment/pod
kubectl delete deployments -n dws deployment-name --force --grace-period=0
kubectl delete pod -n dws pod-name --force --grace-period=0
如果依然无法解决，采取从ETCD数据库删除
ETCD_API=3 etcdctl del /registry/pods/default/pod-name



如果出现删除的namespace一直处于Terminating状态的情况
解决：
	kubectl edit namespaces dws
	找到finalizer的value,删除value,设置为[],即可
如果依然无法解决，采取从ETCD数据库删除
etcdctl del /registry/namespace/NAMESPACENAME	

etcdctl member list    #检查etcd集群节点状态




Kubernetes/k8s 集群组件
Master
	1、API Server
K8S对外的唯一接口，提供HTTP/HTTPS RESTful API，即kubernetes API。
所有的请求都需要经过这个接口进行通信。主要负责接收、校验并响应所有的REST请求，
结果状态被持久存储在etcd当中，所有资源增删改查的唯一入口。

	2、etcd
负责保存k8s 集群的配置信息和各种资源的状态信息，
当数据发生变化时，etcd会快速地通知k8s相关组件。
etcd是一个独立的服务组件，并不隶属于K8S集群。
生产环境当中etcd应该以集群方式运行，以确保服务的可用性。

etcd不仅仅用于提供键值数据存储，而且还为其提供了监听（watch）机制，
用于监听和推送变更。在K8S集群系统中，etcd的键值发生变化会通知倒API Server，
并由其通过watch API向客户端输出。

	3、Controller Manager
负责管理集群各种资源，保证资源处于预期的状态。
Controller Manager由多种controller组成，
包括replication controller、endpoints controller、namespace controller、serviceaccounts controller等 。
由控制器完成的主要功能主要包括生命周期功能和API业务逻辑，具体如下：

生命周期功能：包括Namespace创建和生命周期、Event垃圾回收、Pod终止相关的垃圾回收、级联垃圾回收及Node垃圾回收等。
API业务逻辑：例如，由ReplicaSet执行的Pod扩展等。
	
	4、调度器（Schedule）
资源调度，负责决定将Pod放到哪个Node上运行。
Scheduler在调度时会对集群的结构进行分析，当前各个节点的负载，
以及应用对高可用、性能等方面的需求。


Node
	Node主要负责提供容器的各种依赖环境，并接受Master管理。每个Node有以下几个组件构成。

1、Kubelet
kubelet是node的agent，当Scheduler确定在某个Node上运行Pod后，
会将Pod的具体配置信息（image、volume等）发送给该节点的kubelet，
kubelet会根据这些信息创建和运行容器，并向master报告运行状态。

2、Container Runtime
每个Node都需要提供一个容器运行时（Container Runtime）环境，
它负责下载镜像并运行容器。目前K8S支持的容器运行环境至少包括Docker、RKT、cri-o、Fraki等。

3、Kube-proxy
service在逻辑上代表了后端的多个Pod，外借通过service访问Pod。
service接收到请求就需要kube-proxy完成转发到Pod的。
每个Node都会运行kube-proxy服务，负责将访问的service的TCP/UDP数据流转发到后端的容器，
如果有多个副本，kube-proxy会实现负载均衡，有2种方式：LVS或者Iptables



核心附件
	K8S集群还依赖一组附件组件，通常是由第三方提供的特定应用程序。
	
1、KubeDNS
在K8S集群中调度并运行提供DNS服务的Pod，同一集群内的其他Pod可以使用该DNS服务来解决主机名。
K8S自1.11版本开始默认使用CoreDNS项目来为集群提供服务注册和服务发现的动态名称解析服务。

2、Dashboard
K8S集群的全部功能都要基于Web的UI，来管理集群中的应用和集群自身。

3、Heapster
容器和节点的性能监控与分析系统，它收集并解析多种指标数据，
如资源利用率、生命周期时间，在最新的版本当中，
其主要功能逐渐由Prometheus结合其他的组件进行代替。

4、Ingress Controller
Service是一种工作于4层的负载均衡器，而Ingress是在应用层实现的HTTP(S)的负载均衡。
不过，Ingress资源自身并不能进行流量的穿透，，它仅仅是一组路由规则的集合，
这些规则需要通过Ingress控制器（Ingress Controller）发挥作用。
目前该功能项目大概有：Nginx-ingress、Traefik、Envoy和HAproxy等。


K8S网络模型
	K8S的网络中主要存在4种类型的通信：
①同一Pod内的容器间通信
②各个Pod彼此间的通信
③Pod和Service间的通信
④集群外部流量和Service之间的通信
K8S为Pod和Service资源对象分别使用了各自的专有网络，
Pod网络由K8S的网络插件配置实现，而Service网络则由K8S集群进行指定。


K8S使用的网络插件需要为每个Pod配置至少一个特定的地址，即Pod IP。
Pod IP地址实际存在于某个网卡（可以是虚拟机设备）上。

而Service的地址却是一个虚拟IP地址，没有任何网络接口配置在此地址上，
它由Kube-proxy借助iptables规则或ipvs规则重定向到本地端口，再将其调度到后端的Pod对象。
Service的IP地址是集群提供服务的接口，也称为Cluster IP。

Pod网络和IP由K8S的网络插件负责配置和管理，
具体使用的网络地址可以在管理配置网络插件时进行指定，
如10.244.0.0/16网络。而Cluster网络和IP是由K8S集群负责配置和管理，
如10.96.0.0/12网络。

总结起来，一个K8S集群包含是三个网络。

（1）节点网络：各主机（Master、Node、ETCD等）自身所属的网络，地址配置在主机的网络接口，
		 用于各主机之间的通信，又称为节点网络。
（2）Pod网络：专用于Pod资源对象的网络，它是一个虚拟网络，
     用于为各Pod对象设定IP地址等网络参数，其地址配置在Pod中容器的网络接口上。
     Pod网络需要借助kubenet插件或CNI插件实现。
（3）Service网络：专用于Service资源对象的网络，它也是一个虚拟网络，
     用于为K8S集群之中的Service配置IP地址，但是该地址不会配置在任何主机或容器的网络接口上，
     而是通过Node上的kube-proxy配置为iptables或ipvs规则，
     从而将发往该地址的所有流量调度到后端的各Pod对象之上


K8S核心对象
1、Pod资源对象
Pod资源对象是一种集合了一个或多个应用容器、存储资源、专用ip、
以及支撑运行的其他选项的逻辑组件。如下图：Pod其实就是一个应用程序运行的单一实例，
它通常由共享资源且关系紧密的一个或2多个应用容器组成。

Kubernetes的网络模型要求每个Pod的IP地址同一IP网段，各个Pod之间可以使用IP地址进行通信，
无论这些Pod运行在集群内的哪个节点上，这些Pod对象都类似于运行在同一个局域网内的虚拟机一般。

我们可以将每一个Pod对象类比为一个物理主机或者是虚拟机，那么运行在同一个Pod对象中的多个进程
也就类似于跑在物理主机上的独立进程，而不同的是Pod对象中的各个进程都运行在彼此隔离的容器当中
而各个容器之间共享两种关键性资源：网络和存储卷。

网络：每一个Pod对象都会分配到一个Pod IP地址，
同一个Pod内部的所有容器共享Pod对象的Network和UTS名称空间，
其中包括主机名、IP地址和端口等。因此，这些容器可以通过本地的回环接口lo进行通信，
而在Pod之外的其他组件的通信，则需要使用Service资源对象的Cluster IP+端口完成。

存储卷：用户可以给Pod对象配置一组存储卷资源，
这些资源可以共享给同一个Pod中的所有容器使用，从而完成容器间的数据共享。
存储卷还可以确保在容器终止后被重启，或者是被删除后也能确保数据的持久化存储。


一个Pod代表着某个应用程序的特定实例，如果我们需要去扩展这个应用程序，
那么就意味着需要为该应用程序同时创建多个Pod实例，每个实例都代表着应用程序的一个运行副本。
而这些副本化的Pod对象的创建和管理，都是由一组称为Controller的对象实现，
比如Deployment控制器对象。

当创建Pod时，我们还可以使用Pod Preset对象为Pod注入特定的信息，
比如Configmap、Secret、存储卷、卷挂载、环境变量等。
有了Pod Preset对象，Pod模板的创建就不需要为每个模板显示提供所有信息。

基于预定的期望状态和各个节点的资源可用性，Master会把Pod对象调度至选定的工作节点上运行，
工作节点从指向的镜像仓库进行下载镜像，并在本地的容器运行时环境中启动容器。
Master会将整个集群的状态保存在etcd中，并通过API Server共享给集群的各个组件和客户端。



2、Controller
在K8S的集群设计中，Pod是一个有生命周期的对象。
那么用户通过手工创建或者通过Controller直接创建的Pod对象会被调度器（Scheduler）调度到集群中
的某个工作节点上运行，等到容器应用进程运行结束之后正常终止，随后就会被删除。
而需要注意的是，当节点的资源耗尽或者故障，也会导致Pod对象的回收。

而K8S在这一设计上，使用了控制器实现对一次性的Pod对象进行管理操作。
比如，要确保部署的应用程序的Pod副本数达到用户预期的数目，以及基于Pod模板来重建Pod对象等，
从而实现Pod对象的扩容、缩容、滚动更新和自愈能力。
例如，在某个节点故障，相关的控制器会将运行在该节点上的Pod对象重新调度到其他节点上进行重建。

控制器本身也是一种资源类型，其中包括Replication、Controller、Deployment、StatefulSet、
DaemonSet、Jobs等等，它们都统称为Pod控制器。

job:配置pod对象运行的一次性任务，容器中进程在正常运行结束后不会进行重启，
	  会显示completed状态。
	  实践中，有的任务可能需要不止一次的运行，用户可以配置成串行或者并行的方式
	  单工作队列的串行式job:即把多个一次性的作业方式串行执行多次作业，直至满足期望
	  多工作队列的并行式Job:设置工作队列数，即工作数，每个队列负责运行一个作业
	  
completions: 设置的工作的队列总数，每个队列执行一个job
parallelism: 设置并行执行job数
backoffLimit: 设置job被标记为失败之前重试的次数，默认是6
activeDeadlineSeconds: 设置此次作业的执行时长，超时job将被kill
restartPolicy: 设置未正常结束的job,是否重启,默认是always
               1: always 2:onfailure 3:never
               
cronjob:周期行任务作业计划，控制其运行的时间点及重复运行的方式。
apiVersion: batch/v1beta1
jobTemplate：job控制器模板，用于为cronjob控制器生成job对象；必选字段
schedule：cron格式的作业调度运行时间点；必选字段
concurrencyPolicy：并发执行策略，可用值有Allow、Forbid和Replace，
									 用于定义前一次作业运行尚未完成时是否以及如何运行后一次的作业,
									 默认的是allow
failedJobHistoryLimit：为失败的任务执行保留的历史记录数，默认为1
successfulJobHistoryLimit：为成功的任务执行保留的历史记录数，默认为3
startingDeadlineSeconds：因各种原因缺失执行作业的时间点所导致的启动作业错误的超时时长，
												 会被计入错误历史记录
suspend：是否挂起后续的任务执行，默认为false，对运行中的作业不会产生影响
        
ReplicaSet：用于确保管理的pod对象的副本数目，确保pod健康运行
						kubectl describe replicaset a -n namespace
						获取pod资源变动的相关事件
deployment：为pod和replicaset资源提供声明式更新，具备image的版本
						更新，回滚，暂停和启动，版本记录，状态查看，更新策略
						更新策略：
        			滚动更新：就控制器的Pod数量不断减少，同时新控制器的Pod不断增加，以下两个属性：
        			maxSurge：升级期间存在的总Pod数量最多可超过期望值的个数，可以是数值或百分比
        			maxUnavailabe：升级期间正常可用的Pod数(新旧版本)最多不能其余期望的个数，数值或百分比
daemonset:用于在集群的全部节点同时运行一份指定的POD资源副本，后续加入集群的工作节点也会自动
					创建一个相关的pod对象，当集群移除节点后，资源会回收
					通常运行那些执行系统级操作任务的应用：
					运行集群存储的守护进程：ceph,glusterd
					运行日志守护进程：logstash,fluentd
					运行监控系统的代理守护进程：datadog agent
					只有必须将pod对象运行于固定的几个节点并且需要先于其他pod启动时，
					才有必要使用daemonset控制器，否则就使用deployment控制器		
					
daemonset 和 deployment 的区别：
daemonset 在每个节点上只能运行一个副本
deployment 在节点上可以运行多个副本。			

Pod控制器的定义通常由期望的副本数量、Pod模板、标签选择器组成。
Pod控制器会根据标签选择器来对Pod对象的标签进行匹配筛选，
所有满足选择条件的Pod对象都会被当前控制器进行管理并计入副本总数，
确保数目能够达到预期的状态副本数。

需要注意的是，在实际的应用场景中，
在接收到的请求流量负载低于或接近当前已有Pod副本的承载能力时，
需要我们手动修改Pod控制器中的期望副本数量以实现应用规模的扩容和缩容。
而在集群中部署了HeapSet或者Prometheus的这一类资源监控组件时，
用户还可以通过HPA（HorizontalPodAutoscaler）来计算出合适的Pod副本数量，
并自动地修改Pod控制器中期望的副本数，从而实现应用规模的动态伸缩，提高集群资源的利用率。

K8S集群中的每个节点上都运行着cAdvisor，用于收集容器和节点的CPU、
内存以及磁盘资源的利用率直播数据，这些统计数据由Heapster聚合之后可以通过API server访问。
而HorizontalPodAutoscaler基于这些统计数据监控容器的健康状态并作出扩展决策



3、Service
我们知道Pod对象有Pod IP地址，但是该地址无法确保Pod对象重启或者重建之后保持不变，
这会带来集群中Pod应用间依赖关系维护的麻烦。
比如前段Pod应用无法基于固定的IP地址跟中后端的Pod应用。

而Service资源就是在被访问的Pod对象中添加一个有着固定IP地址的中间层，
客户端向该地址发起访问请求后，由相关的Service资源进行调度并代理到后端的Pod对象。

Service并不是一个具体的组件，而是一个通过规则定义出由多个Pod对象组成而成的逻辑集合，
并附带着访问这组Pod对象的策略。Service对象挑选和关联Pod对象的方式和Pod控制器是一样的，
都是通过标签选择器进行定义


Service IP是一种虚拟IP，也称为Cluster IP，专用于集群内通信，通常使用专有的地址段，
如：10.96.0.0/12网络，各Service对象的IP地址在该范围内由系统动态分配。

集群内的Pod对象可直接请求这类Cluster IP，比如上图中来自Pod client的访问请求，
可以通过Service的Cluster IP作为目标地址进行访问，但是在集群网络中是属于私有的网络地址，
仅仅可以在集群内部访问。

而需要将集群外部的访问引入集群内部的常用方法是通过节点网络进行，其实现方法如下：

通过工作节点的IP地址+端口（Node Port）接入请求。
将该请求代理到相应的Service对象的Cluster IP的服务端口上，
通俗地说：就是工作节点上的端口映射了Service的端口。
由Service对象将该请求转发到后端的Pod对象的Pod IP和 应用程序的监听端口。

因此，类似于上图来自Exxternal Client的集群外部客户端，是无法直接请求该Service的Cluster IP，
而是需要实现经过某一工作节点（如 Node Y）的IP地址，着了请求需要2次转发才能到目标Pod对象。
这一类访问的缺点就是在通信效率上有一定的延时。





[Kubernetes]node节点pod无法启动/节点删除网络重置
错误：
   cni.go:259] Error adding network: failed to set bridge addr: "cni0" already has an IP address different from 10.16.2.1/24
原因：  
   node之前反复添加过,添加之前需要清除下网络
方法：
   重置kubernetes服务，重置网络。删除网络配置，link
   kubeadm reset
   systemctl stop kubelet
   systemctl stop docker
   rm -rf /var/lib/cni/
   rm -rf /var/lib/kubelet/*
   rm -rf /etc/cni/
   ifconfig cni0 down
   ifconfig flannel.1 down
   ifconfig docker0 down
   ip link delete cni0
   ip link delete flannel.1
   systemctl start docker
   
   获取master的join token
   kubeadm token create --print-join-command
   
   加入节点
   kubeadm join --token 55c2c6.2a4bde1bc73a6562 192.168.1.144:6443 --discovery-token-ca-cert-hash sha256:0fdf8cfc6fecc18fded38649a4d9a81d043bf0e4bf57341239250dcc62d2c832

   



docker 部署的zabbxi-web-nginx-mysql服务实现邮件功能
  
方案：
   在zabbix-web-nginx-mysql服务中关联一个centos中已经部署好postfix功能的服务
   调用管理服务实现邮件发送功能
   
   centos 部署邮件
   1：使用Dockerfile部署
   FROM centos
   MAINTAINER dws
   ENV http_proxy 135.251.33.31:80
   ENV https_proxy 135.251.33.31:80
   RUN yum update;yum install -y postfix;yum install -y mailx
   RUN yum install -y yum-utils device-mapper-persistent-data lvm2
   RUN systemctl enable postfix
   CMD ["/bin/bash"]
   一定要用systemctl enable 服务名,不能用systemctl start 服务名,
   虽然基础镜像支持systemd,但是systemctl start会构建失败,
   enable也会开启服务并且开机自启
   
   2：使用创建的image启动容器
   docker run -d -name centos7 --privileged=true centos:7 /usr/sbin/init
   如果出现有Failed to get D-Bus connection: Operation not permitted问题
   必须加上--privileged=true
   
   3：进入容器
    docker exec -it centos7 /bin/bash
    即可启动服务了
    如果出现fatal: parameter inet_interfaces: no local interface found for ::1问题
    变更容器内部文件/etc/postfix/main.cf，将
    
    inet_interfaces = localhost

    inet_protocols = all
    
    改成：
    
    inet_interfaces = all
    
    inet_protocols = all
    
       
/sbin/ip route|awk ‘/default/ { print $3 }’ 
得到的宿主机ip, 加上对应的端口，就可以访问其他容器的服务了





设置集群角色

# 设置 test1 为 master 角色

kubectl label nodes test1 node-role.kubernetes.io/master=

# 设置 test2 为 node 角色

kubectl label nodes 192.168.0.92 node-role.kubernetes.io/node=

# 设置 master 一般情况下不接受负载
kubectl taint nodes test1 node-role.kubernetes.io/master=true:NoSchedule

master运行pod

kubectl taint nodes test1 node-role.kubernetes.io/master-

master不运行pod

kubectl taint nodes test1 node-role.kubernetes.io/master=:NoSchedule

让 master节点恢复不参与POD负载，并将Node上已经存在的Pod驱逐出去的命令为：

kubectl taint nodes <node-name> node-role.kubernetes.io/master=:NoExecute







systemctl status kube-apiserver.service kube-controller-manager.service kube-scheduler.service  | grep active

systemctl status kubelet.service kube-proxy.service | grep active